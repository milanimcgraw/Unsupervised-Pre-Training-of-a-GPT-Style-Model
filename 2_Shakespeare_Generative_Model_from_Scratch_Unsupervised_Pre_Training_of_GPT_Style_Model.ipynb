{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eo_QP1ITFfX2"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjelammcgraw/Unsupervised-Pre-Training-of-a-GPT-Style-Model-Shakespeare-Generative-Model/blob/main/2_Shakespeare_Generative_Model_from_Scratch_Unsupervised_Pre_Training_of_GPT_Style_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Pre-Training of GPT-Style Model\n",
        "\n",
        "In today's notebook, we will be doing an unsupervised pre-training of a GPT-style model.\n",
        "\n",
        "The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!"
      ],
      "metadata": {
        "id": "UWiGVj6njoDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Selection\n",
        "\n",
        "We'll be using a toy dataset called `tinyshakespeare`.\n",
        "\n",
        "You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n",
        "\n",
        "> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus."
      ],
      "metadata": {
        "id": "eHi04aEnkKEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRsEQZy6tgc",
        "outputId": "050a5fbb-67b8-466d-d92a-a1bc0f212277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 649, done.\u001b[K\n",
            "remote: Total 649 (delta 0), reused 0 (delta 0), pack-reused 649\u001b[K\n",
            "Receiving objects: 100% (649/649), 936.45 KiB | 2.57 MiB/s, done.\n",
            "Resolving deltas: 100% (371/371), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dependencies"
      ],
      "metadata": {
        "id": "6l4CqoEDl7ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken requests cohere openai -qU"
      ],
      "metadata": {
        "id": "d_gepPv1Qdj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset!"
      ],
      "metadata": {
        "id": "70hSjXmZmCt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "current_path = \"/data/shakespeare\"\n",
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not os.path.exists(current_path):\n",
        "    os.makedirs(current_path)\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]"
      ],
      "metadata": {
        "id": "T7qRWArUNiZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers"
      ],
      "metadata": {
        "id": "wU9BG2CymU-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers -qU"
      ],
      "metadata": {
        "id": "gFnrwKpQPsYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004f6461-d7bf-4ccf-d74c-b255d349a87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.1/3.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.4/3.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/3.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/3.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/3.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "\"\"\"\n",
        "\n",
        "naive_word_list = input_text.split()"
      ],
      "metadata": {
        "id": "m34NDAGCpiz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting words for frequency."
      ],
      "metadata": {
        "id": "hR8k-2bopqjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "vocab_and_frequencies = defaultdict(int)\n",
        "\n",
        "for word in naive_word_list:\n",
        "  vocab_and_frequencies[\" \".join(list(word))] += 1\n",
        "\n",
        "sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_201bSQpvqD",
        "outputId": "bc770653-9470-4d1f-cf6e-1b3c2c781e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find base vocabulary"
      ],
      "metadata": {
        "id": "NckufSxxp-w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List, Set\n",
        "\n",
        "def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n",
        "  vocab = set()\n",
        "\n",
        "  for word in current_vocab.keys():\n",
        "    for subword in word.split():\n",
        "      vocab.add(subword)\n",
        "\n",
        "  return len(vocab)"
      ],
      "metadata": {
        "id": "BNcjzjDvvKjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf3kCf-WvdBL",
        "outputId": "c0d1d4d1-ba67-4367-a749-95b42b0bbb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start constructing our pairs."
      ],
      "metadata": {
        "id": "OGxrHYmftDTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  pairs = {}\n",
        "\n",
        "  for word, frequency in current_vocab.items():\n",
        "    symbols = word.split()\n",
        "\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pair = (symbols[i], symbols[i + 1])\n",
        "      current_frequency = pairs.get(pair, 0)\n",
        "      pairs[pair] = current_frequency + frequency\n",
        "\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "sTwvfTAErQN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"
      ],
      "metadata": {
        "id": "FudOaKmYv9-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIJfkk7wFYw",
        "outputId": "a75916c1-cc98-40b6-e66f-f95e39179518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('t', 'h'), 11),\n",
              " (('i', 'n'), 10),\n",
              " (('r', 'e'), 8),\n",
              " (('h', 'e'), 8),\n",
              " (('a', 't'), 7)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge pairs into a single token."
      ],
      "metadata": {
        "id": "OqORqdzwsZ6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  vocab_out = {}\n",
        "\n",
        "  pattern = re.escape(' '.join(most_common_pair))\n",
        "  replacement = ''.join(most_common_pair)\n",
        "\n",
        "  for word_in in current_vocab:\n",
        "      word_out = re.sub(pattern, replacement, word_in)\n",
        "      vocab_out[word_out] = current_vocab[word_in]\n",
        "\n",
        "  return vocab_out"
      ],
      "metadata": {
        "id": "L7ohHm2kshoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " new_vocab_and_frequencies = merge_vocab(\n",
        "    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n",
        "    vocab_and_frequencies\n",
        ")"
      ],
      "metadata": {
        "id": "Ab760KKuwzZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0XtvLbpxbSx",
        "outputId": "c16bb1ef-48b6-42a0-9b5f-ea8faef09977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(new_vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO_xegCtxjQf",
        "outputId": "f28783bd-5531-4fba-d986-05046c6a064e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Our Tokenizer\n",
        "\n",
        "\n",
        "1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n",
        "\n",
        "2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so.\n",
        "\n",
        "3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder."
      ],
      "metadata": {
        "id": "BePYCbHly02H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFD, Sequence\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Sequence([NFD()])\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "OrztE09OPosB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(\n",
        "    vocab_size=50000,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\n",
        "      \"<s>\",\n",
        "      \"<pad>\",\n",
        "      \"</s>\",\n",
        "      \"<unk>\",\n",
        "      \"<mask>\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "x9iQVhN3P3RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(files=[input_file_path], trainer=trainer)"
      ],
      "metadata": {
        "id": "LinLHotSP7gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save tokenizer and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"
      ],
      "metadata": {
        "id": "V2JNYiqB2qKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/tokenizer'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.model.save(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk6QjDGHQy2K",
        "outputId": "6ae3f1f8-ff82-48f1-9dc6-054a7d7e4e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOlbggdRFrN",
        "outputId": "5a6a62ed-7b8d-4d1e-ed58-360cacd819b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
      ],
      "metadata": {
        "id": "us1vofdhQ45C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing Inputs"
      ],
      "metadata": {
        "id": "rRUAY4KfK-KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""
      ],
      "metadata": {
        "id": "dnYnFa3fTRLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = tokenizer.tokenize(input_sentence)\n",
        "tokenized_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHY5VufRbBj",
        "outputId": "70b0a827-ce90-430d-a2f9-53f5b7f14c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hark',\n",
              " ',',\n",
              " 'ƒ†my',\n",
              " 'ƒ†name',\n",
              " 'ƒ†be',\n",
              " 'ƒ†Romeo',\n",
              " '!',\n",
              " 'ƒ†I',\n",
              " 'ƒ†am',\n",
              " 'ƒ†but',\n",
              " 'ƒ†a',\n",
              " 'ƒ†beautiful',\n",
              " 'ƒ†summer',\n",
              " \"'s\",\n",
              " 'ƒ†day',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "encoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZrWzQQlTU41",
        "outputId": "8c510246-791b-4c34-9273-a4a16517687e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=False)\n",
        "decoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "oS6lE-NLRnzk",
        "outputId": "654b6ca0-b045-4dd8-f684-9a2fba2c836d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing Dataset\n",
        "\n",
        "Create a dataset we can leverage with the `nanoGPT` library.\n"
      ],
      "metadata": {
        "id": "ji3sF-rA21YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = tokenizer.encode(train_data)\n",
        "val_ids = tokenizer.encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "calHML6JPnCU",
        "outputId": "f0c419ae-95e5-4c5c-c51f-4918fcd544da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 291,284 tokens\n",
            "val has 34,223 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export to bin files\n",
        "data_path = \"/data/shakespeare/\"\n",
        "\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"
      ],
      "metadata": {
        "id": "nKJ1KqiiPkRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9z7ia8AxqEn",
        "outputId": "b18eb4ad-c153-4626-e995-fad4e207ffa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  21,  388,  876,   13,   68, 6804,  373,  153, 2501,  622, 2092,\n",
              "          9,  496,  136,  433,   11,   68,   68,   16,   89,   13,   68,\n",
              "         34, 7882,    9,  433,   11,   68,   68,   21,  388,  876,   13,\n",
              "         68,   40,   73,  252,  227, 3778, 1304,  103,  781,  351,  103,\n",
              "       7504,   15,   68,   68,   16,   89,   13,   68,   33,   97, 5790,\n",
              "         11, 3778,   11,   68,   68,   21,  388,  876,   13,   68,   21,\n",
              "        388,    9,  104,  330, 3317, 1177,  145, 3563, 1766,  103,   80,\n",
              "       1006,   11,   68,   68,   16,   89,   13,   68, 7797,  330,  486,\n",
              "          9,  153,  330,  486,   11,   68,   68,   21,  388,  876,   13,\n",
              "         68], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üèóÔ∏èActivity:\n",
        "\n",
        "Write Python code that will return the first 100 tokens as text.\n",
        "\n",
        "> HINT: An example of this code was used above!"
      ],
      "metadata": {
        "id": "aDbAZt4Lx12n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(train_ids[:100])"
      ],
      "metadata": {
        "id": "za9OzSJDx-dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training The Model"
      ],
      "metadata": {
        "id": "c0I3VrRC3XIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUU2jaalUdqm",
        "outputId": "9260f3b5-81c2-4a3d-e867-7cda5539c06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from the local repo\n",
        "from model import GPTConfig, GPT"
      ],
      "metadata": {
        "id": "weNR37BwUYNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-Parameters\n"
      ],
      "metadata": {
        "id": "kY_vWZG-3uM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = 'out'"
      ],
      "metadata": {
        "id": "viM3qlWt5PVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization"
      ],
      "metadata": {
        "id": "A5iwwrNL5H4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_from = 'scratch'"
      ],
      "metadata": {
        "id": "OK1z2m3C312T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 250\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "eval_only = False\n",
        "always_save_checkpoint = True"
      ],
      "metadata": {
        "id": "MbFN5Ltq4_mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n"
      ],
      "metadata": {
        "id": "a488zaF_4zQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'shakespeare'"
      ],
      "metadata": {
        "id": "_QC7vWXC40Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 1\n",
        "batch_size = 16\n",
        "block_size = 512"
      ],
      "metadata": {
        "id": "EM_ybLPP43Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture"
      ],
      "metadata": {
        "id": "UZ-8bDIY45GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 516\n",
        "dropout = 0.2\n",
        "bias = False"
      ],
      "metadata": {
        "id": "gMyyDBxB6k4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####‚ùìQuestion:\n",
        "\n",
        "How many attention heads (total) will our final network have?\n",
        "\n"
      ],
      "metadata": {
        "id": "FiTyY5Cotwig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_attention_heads = n_layer * n_head\n",
        "print(total_attention_heads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_HlU4fvLDri",
        "outputId": "e335f138-15a3-44f7-f3d5-c2db9020632b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER:** Our final network will have a total of 36 attention heads."
      ],
      "metadata": {
        "id": "k1NLqwFJLeK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer Hyper-Parameters"
      ],
      "metadata": {
        "id": "3NWDTaAz7gwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adamw optimizer\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5_000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "\n",
        "# lr decay settings\n",
        "decay_lr = True\n",
        "weight_decay = 1e-1\n",
        "lr_decay_iters = 5_000\n",
        "min_lr = 1e-4\n",
        "\n",
        "# clipping and warmup\n",
        "grad_clip = 1.0\n",
        "warmup_iters = 100"
      ],
      "metadata": {
        "id": "qe-669jwUptI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backend = 'nccl'\n",
        "device = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = True\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "master_process = True\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiGlMOp8Nux",
        "outputId": "ab7a2e93-2964-4986-f4db-c695c9e73139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 8,192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Settings\n"
      ],
      "metadata": {
        "id": "eKmdfbye-BNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "yh34QGD6VARU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n",
        "\n",
        "1. Set the data path\n",
        "2. Load the dataset we tokenized earlier from the `.bin` we saved\n",
        "3. Define a `get_batch` function"
      ],
      "metadata": {
        "id": "gKeNwYaZ-Zoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join('/data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "tOjaPyJpVEgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
        "x = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])"
      ],
      "metadata": {
        "id": "9_-Y5RZ-yX2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Our randomly selected indices were: {ix}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JDxXph4yh2g",
        "outputId": "2d2f1f7f-e920-4bff-b182-97e42a63af12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our randomly selected indices were: tensor([ 99775, 155569, 263696,  32920,  52919, 231541, 153767, 229238, 136782,\n",
            "        263618,  39008,  14208,  39429, 189430, 194466,  76798])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The first 10 elements of `x` at the first randomly selected index is:\\n{x[0][:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAzHJH-kzK5E",
        "outputId": "e8360777-fbaf-4355-d859-127f3af7e5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 elements of `x` at the first randomly selected index is:\n",
            "tensor([   68,    16,    81,  2358, 19949,   116,   172,  1280,     9,    68])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The first 10 elements of `y` at the first randomly selected index is:\\n{y[0][:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbaF6v8ezkWn",
        "outputId": "41f63416-7157-4b8e-8a05-76db92fa5bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 elements of `y` at the first randomly selected index is:\n",
            "tensor([   16,    81,  2358, 19949,   116,   172,  1280,     9,    68,    16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####‚ùìQuestion:\n",
        "\n",
        "Both `x` and `y` are lists of tokens - as is expected - but what relationship to you notice between `x` and `y`?\n"
      ],
      "metadata": {
        "id": "zOwOEI0Pzntj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER:** In this case, the relationship between 'x' and 'y' is that 'y' is the sequence that follows 'x', and our auto-regressive language model is trying predict each token of 'y' given the tokens in 'x'.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pt3GCbZAOqYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the first component selects a random index from our training data (accounting for our block size)"
      ],
      "metadata": {
        "id": "N62oDfdWy0XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Initialization of Model"
      ],
      "metadata": {
        "id": "EbDlW-68_atH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9"
      ],
      "metadata": {
        "id": "6hsepdVBVzQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = tokenizer.vocab_size\n",
        "meta_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m53DcCdFV0_a",
        "outputId": "42bdd250-653d-47b1-b153-a89b20c71920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20099"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout)"
      ],
      "metadata": {
        "id": "JfIWEbanV7ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xly4iA0V-vF",
        "outputId": "a80d8d27-2a8e-42c9-c36c-4a5d2039605c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size"
      ],
      "metadata": {
        "id": "TrEawNxdWRhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaE3KSTnAtJs",
        "outputId": "acba646a-b8ae-467c-f422-8bf25645ea10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(20099, 516)\n",
              "    (wpe): Embedding(512, 516)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=516, out_features=1548, bias=False)\n",
              "          (c_proj): Linear(in_features=516, out_features=516, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=516, out_features=2064, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=2064, out_features=516, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=516, out_features=20099, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "BNUThRt4WT5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay,\n",
        "    learning_rate,\n",
        "    (beta1, beta2),\n",
        "    device_type\n",
        ")\n",
        "\n",
        "checkpoint = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesGeUnoWViL",
        "outputId": "a3e0090d-e6e9-4993-a738-a050288762ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 29,805,708 parameters\n",
            "num non-decayed parameter tensors: 13, with 6,708 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FNU0T0WXdI",
        "outputId": "c412872f-ad58-412f-d4e1-e9d587099954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "lUB5zVLVWbhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our LR Scheduler\n",
        "\n",
        "![img](https://i.imgur.com/KoFEl0b.png)\n"
      ],
      "metadata": {
        "id": "fLsOpaACDDkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "7-mNpWBSWdHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‚ùìQuestion:\n",
        "\n",
        "What advantages does a learning-rate scheduler have over a static learning rate?\n",
        "\n",
        "Feel free to consult and cite any resources you find!"
      ],
      "metadata": {
        "id": "UV0qN0fDwdOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER:** As opposed to a static learning rate, a learning rate-scheduler is advantageous because of its built in risk mitigation system (stabilization closer to global minima/facilitating hyperparameter tuning). Its flexibility allows faster training and leads to better model convergence and is customizable."
      ],
      "metadata": {
        "id": "nl3exQxODJ-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nDL6s4YT6E",
        "outputId": "acddc54d-3ed1-4b86-a369-d4acbcd16151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Loop"
      ],
      "metadata": {
        "id": "Nhqmxeo0Eg0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch('train')\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model\n",
        "running_mfu = -1.0 # model flops utilization\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHbyEapRWmpc",
        "outputId": "0895a667-d6ba-4335-914f-d3747518f6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 9.9352, val loss 9.9273\n",
            "iter 0: loss 9.9333, time 97771.85ms, mfu -100.00%\n",
            "iter 10: loss 8.3523, time 208.69ms, mfu 2.47%\n",
            "iter 20: loss 7.3770, time 208.44ms, mfu 2.47%\n",
            "iter 30: loss 6.4399, time 209.33ms, mfu 2.47%\n",
            "iter 40: loss 5.8015, time 209.69ms, mfu 2.47%\n",
            "iter 50: loss 5.7303, time 210.07ms, mfu 2.47%\n",
            "iter 60: loss 5.5226, time 210.12ms, mfu 2.47%\n",
            "iter 70: loss 5.2349, time 210.74ms, mfu 2.46%\n",
            "iter 80: loss 5.1079, time 212.94ms, mfu 2.46%\n",
            "iter 90: loss 4.9914, time 213.43ms, mfu 2.45%\n",
            "iter 100: loss 4.5892, time 212.50ms, mfu 2.45%\n",
            "iter 110: loss 4.6100, time 212.17ms, mfu 2.45%\n",
            "iter 120: loss 4.5501, time 212.92ms, mfu 2.45%\n",
            "iter 130: loss 4.5344, time 212.16ms, mfu 2.45%\n",
            "iter 140: loss 4.4540, time 212.93ms, mfu 2.44%\n",
            "iter 150: loss 4.4568, time 212.78ms, mfu 2.44%\n",
            "iter 160: loss 4.4422, time 213.23ms, mfu 2.44%\n",
            "iter 170: loss 4.3045, time 214.52ms, mfu 2.43%\n",
            "iter 180: loss 4.4319, time 214.84ms, mfu 2.43%\n",
            "iter 190: loss 4.2649, time 214.49ms, mfu 2.43%\n",
            "iter 200: loss 4.2327, time 217.20ms, mfu 2.42%\n",
            "iter 210: loss 4.2519, time 216.98ms, mfu 2.42%\n",
            "iter 220: loss 4.2506, time 215.18ms, mfu 2.42%\n",
            "iter 230: loss 4.1970, time 215.66ms, mfu 2.41%\n",
            "iter 240: loss 4.0931, time 216.26ms, mfu 2.41%\n",
            "step 250: train loss 3.9815, val loss 4.9508\n",
            "saving checkpoint to out\n",
            "iter 250: loss 4.0373, time 27732.91ms, mfu 2.17%\n",
            "iter 260: loss 4.0637, time 219.48ms, mfu 2.19%\n",
            "iter 270: loss 3.8842, time 221.57ms, mfu 2.20%\n",
            "iter 280: loss 4.0139, time 222.12ms, mfu 2.21%\n",
            "iter 290: loss 3.8780, time 222.97ms, mfu 2.22%\n",
            "iter 300: loss 3.8923, time 223.34ms, mfu 2.23%\n",
            "iter 310: loss 3.9783, time 222.80ms, mfu 2.24%\n",
            "iter 320: loss 3.8082, time 223.08ms, mfu 2.25%\n",
            "iter 330: loss 3.8076, time 223.71ms, mfu 2.25%\n",
            "iter 340: loss 3.8850, time 224.54ms, mfu 2.26%\n",
            "iter 350: loss 3.7360, time 222.74ms, mfu 2.26%\n",
            "iter 360: loss 3.6240, time 226.19ms, mfu 2.26%\n",
            "iter 370: loss 3.7466, time 227.99ms, mfu 2.26%\n",
            "iter 380: loss 3.7026, time 228.19ms, mfu 2.26%\n",
            "iter 390: loss 3.6978, time 227.85ms, mfu 2.26%\n",
            "iter 400: loss 3.4811, time 227.31ms, mfu 2.26%\n",
            "iter 410: loss 3.6689, time 228.37ms, mfu 2.26%\n",
            "iter 420: loss 3.5417, time 230.77ms, mfu 2.26%\n",
            "iter 430: loss 3.6320, time 231.10ms, mfu 2.26%\n",
            "iter 440: loss 3.5293, time 227.49ms, mfu 2.26%\n",
            "iter 450: loss 3.5417, time 230.28ms, mfu 2.26%\n",
            "iter 460: loss 3.5012, time 228.79ms, mfu 2.26%\n",
            "iter 470: loss 3.5584, time 231.68ms, mfu 2.25%\n",
            "iter 480: loss 3.3524, time 235.31ms, mfu 2.25%\n",
            "iter 490: loss 3.4878, time 236.60ms, mfu 2.24%\n",
            "step 500: train loss 3.3459, val loss 5.1615\n",
            "saving checkpoint to out\n",
            "iter 500: loss 3.3868, time 29437.97ms, mfu 2.02%\n",
            "iter 510: loss 3.5483, time 231.34ms, mfu 2.04%\n",
            "iter 520: loss 3.4405, time 231.16ms, mfu 2.06%\n",
            "iter 530: loss 3.3138, time 237.64ms, mfu 2.07%\n",
            "iter 540: loss 3.4643, time 230.55ms, mfu 2.09%\n",
            "iter 550: loss 3.3001, time 235.97ms, mfu 2.10%\n",
            "iter 560: loss 3.2011, time 238.02ms, mfu 2.10%\n",
            "iter 570: loss 3.1194, time 240.17ms, mfu 2.11%\n",
            "iter 580: loss 3.2235, time 234.19ms, mfu 2.12%\n",
            "iter 590: loss 3.1568, time 232.72ms, mfu 2.13%\n",
            "iter 600: loss 3.2045, time 232.08ms, mfu 2.14%\n",
            "iter 610: loss 3.1660, time 232.51ms, mfu 2.14%\n",
            "iter 620: loss 3.1129, time 234.40ms, mfu 2.15%\n",
            "iter 630: loss 3.2198, time 237.03ms, mfu 2.15%\n",
            "iter 640: loss 3.1919, time 233.03ms, mfu 2.16%\n",
            "iter 650: loss 3.1085, time 235.47ms, mfu 2.16%\n",
            "iter 660: loss 3.1363, time 235.51ms, mfu 2.16%\n",
            "iter 670: loss 3.1025, time 235.95ms, mfu 2.17%\n",
            "iter 680: loss 2.9900, time 235.74ms, mfu 2.17%\n",
            "iter 690: loss 3.0493, time 231.91ms, mfu 2.17%\n",
            "iter 700: loss 2.9270, time 232.92ms, mfu 2.18%\n",
            "iter 710: loss 3.0205, time 230.96ms, mfu 2.18%\n",
            "iter 720: loss 2.8077, time 234.22ms, mfu 2.18%\n",
            "iter 730: loss 2.9583, time 238.16ms, mfu 2.18%\n",
            "iter 740: loss 2.8777, time 235.88ms, mfu 2.18%\n",
            "step 750: train loss 2.6812, val loss 5.4646\n",
            "saving checkpoint to out\n",
            "iter 750: loss 2.8394, time 29168.69ms, mfu 1.97%\n",
            "iter 760: loss 2.8454, time 232.19ms, mfu 1.99%\n",
            "iter 770: loss 2.7440, time 229.55ms, mfu 2.02%\n",
            "iter 780: loss 2.7710, time 229.51ms, mfu 2.04%\n",
            "iter 790: loss 2.6739, time 229.61ms, mfu 2.06%\n",
            "iter 800: loss 2.9756, time 235.51ms, mfu 2.07%\n",
            "iter 810: loss 2.7831, time 235.15ms, mfu 2.09%\n",
            "iter 820: loss 2.7702, time 233.91ms, mfu 2.10%\n",
            "iter 830: loss 2.5908, time 236.96ms, mfu 2.10%\n",
            "iter 840: loss 2.6935, time 238.43ms, mfu 2.11%\n",
            "iter 850: loss 2.8893, time 239.80ms, mfu 2.11%\n",
            "iter 860: loss 2.6482, time 240.99ms, mfu 2.12%\n",
            "iter 870: loss 2.5060, time 239.49ms, mfu 2.12%\n",
            "iter 880: loss 2.5264, time 239.49ms, mfu 2.12%\n",
            "iter 890: loss 2.4970, time 240.82ms, mfu 2.13%\n",
            "iter 900: loss 2.3944, time 234.49ms, mfu 2.13%\n",
            "iter 910: loss 2.5317, time 234.91ms, mfu 2.14%\n",
            "iter 920: loss 2.4794, time 235.01ms, mfu 2.14%\n",
            "iter 930: loss 2.3673, time 234.88ms, mfu 2.15%\n",
            "iter 940: loss 2.5209, time 235.33ms, mfu 2.15%\n",
            "iter 950: loss 2.3984, time 231.65ms, mfu 2.16%\n",
            "iter 960: loss 2.1165, time 232.39ms, mfu 2.17%\n",
            "iter 970: loss 2.1958, time 230.06ms, mfu 2.17%\n",
            "iter 980: loss 2.2744, time 233.01ms, mfu 2.18%\n",
            "iter 990: loss 2.1369, time 230.11ms, mfu 2.18%\n",
            "step 1000: train loss 1.9180, val loss 5.8455\n",
            "saving checkpoint to out\n",
            "iter 1000: loss 2.4399, time 29006.72ms, mfu 1.97%\n",
            "iter 1010: loss 2.2001, time 228.02ms, mfu 2.00%\n",
            "iter 1020: loss 2.2594, time 227.29ms, mfu 2.02%\n",
            "iter 1030: loss 2.3239, time 229.75ms, mfu 2.05%\n",
            "iter 1040: loss 1.9787, time 231.46ms, mfu 2.06%\n",
            "iter 1050: loss 1.9900, time 231.15ms, mfu 2.08%\n",
            "iter 1060: loss 2.1077, time 235.62ms, mfu 2.09%\n",
            "iter 1070: loss 1.9700, time 236.73ms, mfu 2.10%\n",
            "iter 1080: loss 2.1339, time 238.59ms, mfu 2.11%\n",
            "iter 1090: loss 1.8206, time 240.36ms, mfu 2.11%\n",
            "iter 1100: loss 2.0103, time 244.26ms, mfu 2.11%\n",
            "iter 1110: loss 1.9877, time 243.59ms, mfu 2.11%\n",
            "iter 1120: loss 1.9279, time 240.35ms, mfu 2.11%\n",
            "iter 1130: loss 1.8322, time 237.31ms, mfu 2.12%\n",
            "iter 1140: loss 1.8483, time 237.55ms, mfu 2.12%\n",
            "iter 1150: loss 1.8934, time 238.63ms, mfu 2.13%\n",
            "iter 1160: loss 1.9732, time 233.62ms, mfu 2.14%\n",
            "iter 1170: loss 1.9356, time 237.51ms, mfu 2.14%\n",
            "iter 1180: loss 1.7146, time 234.28ms, mfu 2.15%\n",
            "iter 1190: loss 1.8974, time 232.84ms, mfu 2.15%\n",
            "iter 1200: loss 1.8483, time 236.20ms, mfu 2.16%\n",
            "iter 1210: loss 1.9165, time 233.74ms, mfu 2.16%\n",
            "iter 1220: loss 1.8050, time 231.53ms, mfu 2.17%\n",
            "iter 1230: loss 1.7599, time 225.96ms, mfu 2.18%\n",
            "iter 1240: loss 1.6818, time 228.36ms, mfu 2.19%\n",
            "step 1250: train loss 1.3152, val loss 6.3722\n",
            "saving checkpoint to out\n",
            "iter 1250: loss 1.6531, time 28991.54ms, mfu 1.97%\n",
            "iter 1260: loss 1.6669, time 229.50ms, mfu 2.00%\n",
            "iter 1270: loss 1.7845, time 230.46ms, mfu 2.02%\n",
            "iter 1280: loss 1.5612, time 230.12ms, mfu 2.04%\n",
            "iter 1290: loss 1.5920, time 233.69ms, mfu 2.06%\n",
            "iter 1300: loss 1.6728, time 232.82ms, mfu 2.07%\n",
            "iter 1310: loss 1.6878, time 237.40ms, mfu 2.08%\n",
            "iter 1320: loss 1.5237, time 240.88ms, mfu 2.09%\n",
            "iter 1330: loss 1.5301, time 238.38ms, mfu 2.10%\n",
            "iter 1340: loss 1.4958, time 237.59ms, mfu 2.10%\n",
            "iter 1350: loss 1.5984, time 241.62ms, mfu 2.11%\n",
            "iter 1360: loss 1.6180, time 242.48ms, mfu 2.11%\n",
            "iter 1370: loss 1.4878, time 239.53ms, mfu 2.11%\n",
            "iter 1380: loss 1.5540, time 239.69ms, mfu 2.12%\n",
            "iter 1390: loss 1.5385, time 237.14ms, mfu 2.12%\n",
            "iter 1400: loss 1.4502, time 239.39ms, mfu 2.13%\n",
            "iter 1410: loss 1.4533, time 234.18ms, mfu 2.13%\n",
            "iter 1420: loss 1.5093, time 237.82ms, mfu 2.14%\n",
            "iter 1430: loss 1.4801, time 235.11ms, mfu 2.14%\n",
            "iter 1440: loss 1.4103, time 232.14ms, mfu 2.15%\n",
            "iter 1450: loss 1.5479, time 225.30ms, mfu 2.16%\n",
            "iter 1460: loss 1.3283, time 234.65ms, mfu 2.17%\n",
            "iter 1470: loss 1.4267, time 228.13ms, mfu 2.18%\n",
            "iter 1480: loss 1.3454, time 227.33ms, mfu 2.19%\n",
            "iter 1490: loss 1.4568, time 228.65ms, mfu 2.19%\n",
            "step 1500: train loss 0.9240, val loss 6.8907\n",
            "saving checkpoint to out\n",
            "iter 1500: loss 1.3558, time 24110.44ms, mfu 1.98%\n",
            "iter 1510: loss 1.3248, time 234.95ms, mfu 2.00%\n",
            "iter 1520: loss 1.4041, time 230.85ms, mfu 2.02%\n",
            "iter 1530: loss 1.1991, time 233.47ms, mfu 2.04%\n",
            "iter 1540: loss 1.2690, time 232.05ms, mfu 2.06%\n",
            "iter 1550: loss 1.2366, time 240.12ms, mfu 2.07%\n",
            "iter 1560: loss 1.2242, time 234.19ms, mfu 2.08%\n",
            "iter 1570: loss 1.2307, time 238.12ms, mfu 2.09%\n",
            "iter 1580: loss 1.2177, time 236.63ms, mfu 2.10%\n",
            "iter 1590: loss 1.1988, time 230.81ms, mfu 2.11%\n",
            "iter 1600: loss 1.2568, time 232.31ms, mfu 2.12%\n",
            "iter 1610: loss 1.2876, time 236.88ms, mfu 2.13%\n",
            "iter 1620: loss 1.2306, time 233.82ms, mfu 2.13%\n",
            "iter 1630: loss 1.2026, time 231.93ms, mfu 2.14%\n",
            "iter 1640: loss 1.1509, time 232.12ms, mfu 2.15%\n",
            "iter 1650: loss 1.1735, time 234.06ms, mfu 2.16%\n",
            "iter 1660: loss 1.1483, time 234.01ms, mfu 2.16%\n",
            "iter 1670: loss 1.1372, time 234.11ms, mfu 2.17%\n",
            "iter 1680: loss 1.1399, time 231.95ms, mfu 2.17%\n",
            "iter 1690: loss 1.2689, time 233.03ms, mfu 2.17%\n",
            "iter 1700: loss 1.1359, time 232.67ms, mfu 2.18%\n",
            "iter 1710: loss 1.0084, time 233.29ms, mfu 2.18%\n",
            "iter 1720: loss 1.0761, time 233.21ms, mfu 2.18%\n",
            "iter 1730: loss 1.1050, time 235.41ms, mfu 2.19%\n",
            "iter 1740: loss 1.0668, time 231.28ms, mfu 2.19%\n",
            "step 1750: train loss 0.6646, val loss 7.1944\n",
            "saving checkpoint to out\n",
            "iter 1750: loss 1.0556, time 26550.46ms, mfu 1.97%\n",
            "iter 1760: loss 1.0886, time 227.26ms, mfu 2.00%\n",
            "iter 1770: loss 1.0827, time 227.46ms, mfu 2.03%\n",
            "iter 1780: loss 1.0096, time 233.90ms, mfu 2.05%\n",
            "iter 1790: loss 1.1339, time 233.47ms, mfu 2.06%\n",
            "iter 1800: loss 1.1234, time 228.92ms, mfu 2.08%\n",
            "iter 1810: loss 0.9721, time 232.15ms, mfu 2.10%\n",
            "iter 1820: loss 1.0951, time 236.36ms, mfu 2.10%\n",
            "iter 1830: loss 0.9838, time 231.85ms, mfu 2.12%\n",
            "iter 1840: loss 1.0384, time 232.27ms, mfu 2.13%\n",
            "iter 1850: loss 0.9995, time 233.57ms, mfu 2.13%\n",
            "iter 1860: loss 1.0143, time 232.33ms, mfu 2.14%\n",
            "iter 1870: loss 0.9096, time 231.43ms, mfu 2.15%\n",
            "iter 1880: loss 0.9703, time 233.99ms, mfu 2.16%\n",
            "iter 1890: loss 0.9839, time 235.84ms, mfu 2.16%\n",
            "iter 1900: loss 0.9798, time 233.80ms, mfu 2.16%\n",
            "iter 1910: loss 0.9426, time 238.94ms, mfu 2.16%\n",
            "iter 1920: loss 0.9938, time 238.23ms, mfu 2.16%\n",
            "iter 1930: loss 0.9688, time 235.18ms, mfu 2.17%\n",
            "iter 1940: loss 0.9865, time 234.71ms, mfu 2.17%\n",
            "iter 1950: loss 0.9350, time 236.74ms, mfu 2.17%\n",
            "iter 1960: loss 0.9540, time 232.85ms, mfu 2.17%\n",
            "iter 1970: loss 0.9284, time 234.46ms, mfu 2.18%\n",
            "iter 1980: loss 0.9178, time 232.28ms, mfu 2.18%\n",
            "iter 1990: loss 0.9184, time 234.25ms, mfu 2.18%\n",
            "step 2000: train loss 0.5040, val loss 7.5518\n",
            "saving checkpoint to out\n",
            "iter 2000: loss 0.8806, time 29160.97ms, mfu 1.97%\n",
            "iter 2010: loss 0.9202, time 226.97ms, mfu 2.00%\n",
            "iter 2020: loss 0.8835, time 227.25ms, mfu 2.02%\n",
            "iter 2030: loss 0.8104, time 228.78ms, mfu 2.05%\n",
            "iter 2040: loss 0.8016, time 229.65ms, mfu 2.07%\n",
            "iter 2050: loss 0.8490, time 232.39ms, mfu 2.08%\n",
            "iter 2060: loss 0.8606, time 231.79ms, mfu 2.10%\n",
            "iter 2070: loss 0.7823, time 241.81ms, mfu 2.10%\n",
            "iter 2080: loss 0.8175, time 239.26ms, mfu 2.10%\n",
            "iter 2090: loss 0.7981, time 237.25ms, mfu 2.11%\n",
            "iter 2100: loss 0.8452, time 237.67ms, mfu 2.12%\n",
            "iter 2110: loss 0.8489, time 240.81ms, mfu 2.12%\n",
            "iter 2120: loss 0.8688, time 238.37ms, mfu 2.12%\n",
            "iter 2130: loss 0.8517, time 234.25ms, mfu 2.13%\n",
            "iter 2140: loss 0.8759, time 240.37ms, mfu 2.13%\n",
            "iter 2150: loss 0.7618, time 242.34ms, mfu 2.13%\n",
            "iter 2160: loss 0.7812, time 235.19ms, mfu 2.14%\n",
            "iter 2170: loss 0.8858, time 237.22ms, mfu 2.14%\n",
            "iter 2180: loss 0.8541, time 234.58ms, mfu 2.15%\n",
            "iter 2190: loss 0.8143, time 232.36ms, mfu 2.15%\n",
            "iter 2200: loss 0.7483, time 233.24ms, mfu 2.16%\n",
            "iter 2210: loss 0.7561, time 232.19ms, mfu 2.17%\n",
            "iter 2220: loss 0.7777, time 229.58ms, mfu 2.17%\n",
            "iter 2230: loss 0.7132, time 228.36ms, mfu 2.18%\n",
            "iter 2240: loss 0.7465, time 227.98ms, mfu 2.19%\n",
            "step 2250: train loss 0.3936, val loss 7.8938\n",
            "saving checkpoint to out\n",
            "iter 2250: loss 0.7554, time 29038.26ms, mfu 1.97%\n",
            "iter 2260: loss 0.7402, time 227.70ms, mfu 2.00%\n",
            "iter 2270: loss 0.7195, time 230.43ms, mfu 2.03%\n",
            "iter 2280: loss 0.6900, time 230.00ms, mfu 2.05%\n",
            "iter 2290: loss 0.7271, time 234.81ms, mfu 2.06%\n",
            "iter 2300: loss 0.7450, time 233.38ms, mfu 2.08%\n",
            "iter 2310: loss 0.6873, time 235.78ms, mfu 2.09%\n",
            "iter 2320: loss 0.6994, time 234.29ms, mfu 2.10%\n",
            "iter 2330: loss 0.6974, time 234.89ms, mfu 2.11%\n",
            "iter 2340: loss 0.7229, time 241.34ms, mfu 2.11%\n",
            "iter 2350: loss 0.7185, time 241.86ms, mfu 2.11%\n",
            "iter 2360: loss 0.7309, time 242.28ms, mfu 2.11%\n",
            "iter 2370: loss 0.7137, time 239.06ms, mfu 2.12%\n",
            "iter 2380: loss 0.6689, time 238.13ms, mfu 2.12%\n",
            "iter 2390: loss 0.6898, time 237.34ms, mfu 2.13%\n",
            "iter 2400: loss 0.6745, time 239.02ms, mfu 2.13%\n",
            "iter 2410: loss 0.7896, time 238.72ms, mfu 2.13%\n",
            "iter 2420: loss 0.6409, time 233.43ms, mfu 2.14%\n",
            "iter 2430: loss 0.6378, time 230.23ms, mfu 2.15%\n",
            "iter 2440: loss 0.6843, time 233.09ms, mfu 2.16%\n",
            "iter 2450: loss 0.6543, time 233.55ms, mfu 2.16%\n",
            "iter 2460: loss 0.5896, time 229.48ms, mfu 2.17%\n",
            "iter 2470: loss 0.6781, time 232.06ms, mfu 2.18%\n",
            "iter 2480: loss 0.6697, time 226.00ms, mfu 2.19%\n",
            "iter 2490: loss 0.6420, time 229.77ms, mfu 2.19%\n",
            "step 2500: train loss 0.3007, val loss 8.1212\n",
            "saving checkpoint to out\n",
            "iter 2500: loss 0.6533, time 28978.31ms, mfu 1.97%\n",
            "iter 2510: loss 0.5865, time 228.76ms, mfu 2.00%\n",
            "iter 2520: loss 0.6171, time 228.85ms, mfu 2.03%\n",
            "iter 2530: loss 0.5582, time 227.86ms, mfu 2.05%\n",
            "iter 2540: loss 0.6890, time 230.00ms, mfu 2.07%\n",
            "iter 2550: loss 0.5832, time 232.26ms, mfu 2.08%\n",
            "iter 2560: loss 0.5893, time 232.66ms, mfu 2.10%\n",
            "iter 2570: loss 0.6211, time 234.94ms, mfu 2.11%\n",
            "iter 2580: loss 0.6314, time 238.63ms, mfu 2.11%\n",
            "iter 2590: loss 0.5571, time 239.32ms, mfu 2.12%\n",
            "iter 2600: loss 0.5767, time 242.19ms, mfu 2.12%\n",
            "iter 2610: loss 0.5859, time 238.75ms, mfu 2.12%\n",
            "iter 2620: loss 0.5704, time 240.87ms, mfu 2.12%\n",
            "iter 2630: loss 0.5662, time 241.19ms, mfu 2.13%\n",
            "iter 2640: loss 0.5536, time 238.35ms, mfu 2.13%\n",
            "iter 2650: loss 0.5674, time 235.64ms, mfu 2.13%\n",
            "iter 2660: loss 0.5667, time 234.18ms, mfu 2.14%\n",
            "iter 2670: loss 0.5580, time 231.61ms, mfu 2.15%\n",
            "iter 2680: loss 0.6203, time 229.65ms, mfu 2.16%\n",
            "iter 2690: loss 0.6030, time 229.37ms, mfu 2.17%\n",
            "iter 2700: loss 0.5373, time 230.16ms, mfu 2.18%\n",
            "iter 2710: loss 0.5451, time 225.11ms, mfu 2.19%\n",
            "iter 2720: loss 0.5566, time 230.30ms, mfu 2.19%\n",
            "iter 2730: loss 0.4996, time 226.20ms, mfu 2.20%\n",
            "iter 2740: loss 0.5113, time 231.58ms, mfu 2.20%\n",
            "step 2750: train loss 0.2353, val loss 8.3492\n",
            "saving checkpoint to out\n",
            "iter 2750: loss 0.5658, time 28936.75ms, mfu 1.98%\n",
            "iter 2760: loss 0.5535, time 226.83ms, mfu 2.01%\n",
            "iter 2770: loss 0.4985, time 227.96ms, mfu 2.04%\n",
            "iter 2780: loss 0.5528, time 232.95ms, mfu 2.06%\n",
            "iter 2790: loss 0.4836, time 230.99ms, mfu 2.07%\n",
            "iter 2800: loss 0.4930, time 230.25ms, mfu 2.09%\n",
            "iter 2810: loss 0.5476, time 238.66ms, mfu 2.10%\n",
            "iter 2820: loss 0.4778, time 230.43ms, mfu 2.11%\n",
            "iter 2830: loss 0.4556, time 238.95ms, mfu 2.12%\n",
            "iter 2840: loss 0.5200, time 241.45ms, mfu 2.12%\n",
            "iter 2850: loss 0.4659, time 242.84ms, mfu 2.12%\n",
            "iter 2860: loss 0.4384, time 243.77ms, mfu 2.12%\n",
            "iter 2870: loss 0.4791, time 240.61ms, mfu 2.12%\n",
            "iter 2880: loss 0.4651, time 234.97ms, mfu 2.13%\n",
            "iter 2890: loss 0.4652, time 233.13ms, mfu 2.14%\n",
            "iter 2900: loss 0.4652, time 235.04ms, mfu 2.14%\n",
            "iter 2910: loss 0.4379, time 234.00ms, mfu 2.15%\n",
            "iter 2920: loss 0.4488, time 235.25ms, mfu 2.15%\n",
            "iter 2930: loss 0.4977, time 228.46ms, mfu 2.16%\n",
            "iter 2940: loss 0.4869, time 236.76ms, mfu 2.16%\n",
            "iter 2950: loss 0.4687, time 228.17ms, mfu 2.17%\n",
            "iter 2960: loss 0.4614, time 228.30ms, mfu 2.18%\n",
            "iter 2970: loss 0.4702, time 229.09ms, mfu 2.19%\n",
            "iter 2980: loss 0.4468, time 230.21ms, mfu 2.19%\n",
            "iter 2990: loss 0.4461, time 229.04ms, mfu 2.20%\n",
            "step 3000: train loss 0.1885, val loss 8.5472\n",
            "saving checkpoint to out\n",
            "iter 3000: loss 0.4653, time 29042.16ms, mfu 1.98%\n",
            "iter 3010: loss 0.4732, time 227.13ms, mfu 2.01%\n",
            "iter 3020: loss 0.4461, time 232.25ms, mfu 2.03%\n",
            "iter 3030: loss 0.4377, time 230.35ms, mfu 2.05%\n",
            "iter 3040: loss 0.4731, time 228.95ms, mfu 2.07%\n",
            "iter 3050: loss 0.4366, time 224.55ms, mfu 2.09%\n",
            "iter 3060: loss 0.4134, time 236.45ms, mfu 2.10%\n",
            "iter 3070: loss 0.4273, time 239.77ms, mfu 2.11%\n",
            "iter 3080: loss 0.4110, time 235.63ms, mfu 2.12%\n",
            "iter 3090: loss 0.3833, time 236.43ms, mfu 2.12%\n",
            "iter 3100: loss 0.4447, time 241.54ms, mfu 2.12%\n",
            "iter 3110: loss 0.4133, time 242.99ms, mfu 2.12%\n",
            "iter 3120: loss 0.4469, time 237.12ms, mfu 2.13%\n",
            "iter 3130: loss 0.4279, time 236.66ms, mfu 2.13%\n",
            "iter 3140: loss 0.4536, time 238.00ms, mfu 2.14%\n",
            "iter 3150: loss 0.4087, time 235.23ms, mfu 2.14%\n",
            "iter 3160: loss 0.3979, time 235.55ms, mfu 2.15%\n",
            "iter 3170: loss 0.4270, time 233.32ms, mfu 2.15%\n",
            "iter 3180: loss 0.3854, time 234.95ms, mfu 2.16%\n",
            "iter 3190: loss 0.3860, time 232.40ms, mfu 2.16%\n",
            "iter 3200: loss 0.4158, time 233.56ms, mfu 2.17%\n",
            "iter 3210: loss 0.4488, time 227.57ms, mfu 2.18%\n",
            "iter 3220: loss 0.3483, time 230.87ms, mfu 2.18%\n",
            "iter 3230: loss 0.4137, time 227.96ms, mfu 2.19%\n",
            "iter 3240: loss 0.3921, time 231.03ms, mfu 2.19%\n",
            "step 3250: train loss 0.1508, val loss 8.6524\n",
            "saving checkpoint to out\n",
            "iter 3250: loss 0.4170, time 25170.00ms, mfu 1.98%\n",
            "iter 3260: loss 0.3459, time 231.95ms, mfu 2.00%\n",
            "iter 3270: loss 0.3372, time 236.01ms, mfu 2.02%\n",
            "iter 3280: loss 0.3805, time 229.52ms, mfu 2.04%\n",
            "iter 3290: loss 0.3761, time 232.55ms, mfu 2.06%\n",
            "iter 3300: loss 0.3765, time 233.92ms, mfu 2.07%\n",
            "iter 3310: loss 0.3962, time 231.98ms, mfu 2.09%\n",
            "iter 3320: loss 0.3664, time 235.71ms, mfu 2.10%\n",
            "iter 3330: loss 0.3659, time 236.04ms, mfu 2.11%\n",
            "iter 3340: loss 0.3821, time 237.45ms, mfu 2.11%\n",
            "iter 3350: loss 0.3783, time 234.41ms, mfu 2.12%\n",
            "iter 3360: loss 0.3885, time 234.63ms, mfu 2.13%\n",
            "iter 3370: loss 0.3683, time 237.58ms, mfu 2.13%\n",
            "iter 3380: loss 0.3938, time 230.52ms, mfu 2.14%\n",
            "iter 3390: loss 0.3562, time 234.43ms, mfu 2.15%\n",
            "iter 3400: loss 0.3311, time 235.84ms, mfu 2.15%\n",
            "iter 3410: loss 0.3624, time 233.72ms, mfu 2.16%\n",
            "iter 3420: loss 0.3539, time 235.21ms, mfu 2.16%\n",
            "iter 3430: loss 0.3386, time 235.28ms, mfu 2.16%\n",
            "iter 3440: loss 0.3248, time 231.52ms, mfu 2.17%\n",
            "iter 3450: loss 0.3410, time 232.92ms, mfu 2.17%\n",
            "iter 3460: loss 0.3577, time 229.02ms, mfu 2.18%\n",
            "iter 3470: loss 0.3189, time 230.06ms, mfu 2.19%\n",
            "iter 3480: loss 0.3243, time 232.69ms, mfu 2.19%\n",
            "iter 3490: loss 0.3367, time 232.02ms, mfu 2.19%\n",
            "step 3500: train loss 0.1266, val loss 8.7879\n",
            "saving checkpoint to out\n",
            "iter 3500: loss 0.3050, time 29175.00ms, mfu 1.98%\n",
            "iter 3510: loss 0.3030, time 228.03ms, mfu 2.00%\n",
            "iter 3520: loss 0.3545, time 223.66ms, mfu 2.03%\n",
            "iter 3530: loss 0.3022, time 226.13ms, mfu 2.06%\n",
            "iter 3540: loss 0.3204, time 231.29ms, mfu 2.08%\n",
            "iter 3550: loss 0.3118, time 229.10ms, mfu 2.09%\n",
            "iter 3560: loss 0.3322, time 230.70ms, mfu 2.11%\n",
            "iter 3570: loss 0.3183, time 237.17ms, mfu 2.11%\n",
            "iter 3580: loss 0.3367, time 240.25ms, mfu 2.12%\n",
            "iter 3590: loss 0.3064, time 238.32ms, mfu 2.12%\n",
            "iter 3600: loss 0.3126, time 240.97ms, mfu 2.12%\n",
            "iter 3610: loss 0.2988, time 240.96ms, mfu 2.13%\n",
            "iter 3620: loss 0.3268, time 238.56ms, mfu 2.13%\n",
            "iter 3630: loss 0.3012, time 239.72ms, mfu 2.13%\n",
            "iter 3640: loss 0.2944, time 233.45ms, mfu 2.14%\n",
            "iter 3650: loss 0.3008, time 236.03ms, mfu 2.14%\n",
            "iter 3660: loss 0.2772, time 234.23ms, mfu 2.15%\n",
            "iter 3670: loss 0.2960, time 231.04ms, mfu 2.16%\n",
            "iter 3680: loss 0.3245, time 235.15ms, mfu 2.16%\n",
            "iter 3690: loss 0.3172, time 237.50ms, mfu 2.16%\n",
            "iter 3700: loss 0.2939, time 232.75ms, mfu 2.17%\n",
            "iter 3710: loss 0.2945, time 231.70ms, mfu 2.17%\n",
            "iter 3720: loss 0.2748, time 229.63ms, mfu 2.18%\n",
            "iter 3730: loss 0.2911, time 226.86ms, mfu 2.19%\n",
            "iter 3740: loss 0.2872, time 228.86ms, mfu 2.20%\n",
            "step 3750: train loss 0.1089, val loss 8.9115\n",
            "saving checkpoint to out\n",
            "iter 3750: loss 0.3024, time 26714.39ms, mfu 1.98%\n",
            "iter 3760: loss 0.3063, time 229.23ms, mfu 2.00%\n",
            "iter 3770: loss 0.2881, time 232.71ms, mfu 2.03%\n",
            "iter 3780: loss 0.2852, time 234.35ms, mfu 2.04%\n",
            "iter 3790: loss 0.2834, time 228.98ms, mfu 2.06%\n",
            "iter 3800: loss 0.2915, time 228.41ms, mfu 2.08%\n",
            "iter 3810: loss 0.2899, time 232.99ms, mfu 2.10%\n",
            "iter 3820: loss 0.2818, time 235.89ms, mfu 2.11%\n",
            "iter 3830: loss 0.2892, time 239.75ms, mfu 2.11%\n",
            "iter 3840: loss 0.2780, time 238.39ms, mfu 2.11%\n",
            "iter 3850: loss 0.2803, time 234.40ms, mfu 2.12%\n",
            "iter 3860: loss 0.2416, time 239.24ms, mfu 2.13%\n",
            "iter 3870: loss 0.2697, time 238.61ms, mfu 2.13%\n",
            "iter 3880: loss 0.2674, time 237.76ms, mfu 2.13%\n",
            "iter 3890: loss 0.2767, time 235.51ms, mfu 2.14%\n",
            "iter 3900: loss 0.2694, time 235.35ms, mfu 2.14%\n",
            "iter 3910: loss 0.2682, time 235.72ms, mfu 2.15%\n",
            "iter 3920: loss 0.2700, time 232.71ms, mfu 2.16%\n",
            "iter 3930: loss 0.2595, time 236.33ms, mfu 2.16%\n",
            "iter 3940: loss 0.2621, time 233.07ms, mfu 2.16%\n",
            "iter 3950: loss 0.2653, time 231.29ms, mfu 2.17%\n",
            "iter 3960: loss 0.2649, time 234.27ms, mfu 2.17%\n",
            "iter 3970: loss 0.2559, time 232.70ms, mfu 2.18%\n",
            "iter 3980: loss 0.2342, time 230.05ms, mfu 2.18%\n",
            "iter 3990: loss 0.2314, time 231.51ms, mfu 2.19%\n",
            "step 4000: train loss 0.0971, val loss 8.9733\n",
            "saving checkpoint to out\n",
            "iter 4000: loss 0.2566, time 29185.09ms, mfu 1.97%\n",
            "iter 4010: loss 0.2486, time 228.82ms, mfu 2.00%\n",
            "iter 4020: loss 0.2678, time 228.62ms, mfu 2.02%\n",
            "iter 4030: loss 0.2489, time 228.23ms, mfu 2.05%\n",
            "iter 4040: loss 0.2470, time 230.72ms, mfu 2.07%\n",
            "iter 4050: loss 0.2604, time 231.61ms, mfu 2.08%\n",
            "iter 4060: loss 0.2653, time 233.68ms, mfu 2.09%\n",
            "iter 4070: loss 0.2514, time 234.75ms, mfu 2.10%\n",
            "iter 4080: loss 0.2531, time 233.11ms, mfu 2.12%\n",
            "iter 4090: loss 0.2718, time 233.70ms, mfu 2.12%\n",
            "iter 4100: loss 0.2400, time 239.57ms, mfu 2.13%\n",
            "iter 4110: loss 0.2259, time 242.05ms, mfu 2.13%\n",
            "iter 4120: loss 0.2601, time 239.81ms, mfu 2.13%\n",
            "iter 4130: loss 0.2614, time 232.53ms, mfu 2.14%\n",
            "iter 4140: loss 0.2638, time 238.59ms, mfu 2.14%\n",
            "iter 4150: loss 0.2566, time 234.85ms, mfu 2.15%\n",
            "iter 4160: loss 0.2528, time 230.07ms, mfu 2.16%\n",
            "iter 4170: loss 0.2280, time 237.45ms, mfu 2.16%\n",
            "iter 4180: loss 0.2410, time 234.30ms, mfu 2.16%\n",
            "iter 4190: loss 0.2373, time 230.85ms, mfu 2.17%\n",
            "iter 4200: loss 0.2686, time 237.29ms, mfu 2.17%\n",
            "iter 4210: loss 0.2500, time 234.53ms, mfu 2.17%\n",
            "iter 4220: loss 0.2435, time 230.91ms, mfu 2.18%\n",
            "iter 4230: loss 0.2305, time 233.96ms, mfu 2.18%\n",
            "iter 4240: loss 0.2566, time 234.84ms, mfu 2.18%\n",
            "step 4250: train loss 0.0890, val loss 9.0894\n",
            "saving checkpoint to out\n",
            "iter 4250: loss 0.2302, time 29025.46ms, mfu 1.97%\n",
            "iter 4260: loss 0.2244, time 228.57ms, mfu 1.99%\n",
            "iter 4270: loss 0.2461, time 226.26ms, mfu 2.02%\n",
            "iter 4280: loss 0.2519, time 230.15ms, mfu 2.04%\n",
            "iter 4290: loss 0.2056, time 232.76ms, mfu 2.06%\n",
            "iter 4300: loss 0.2339, time 230.53ms, mfu 2.08%\n",
            "iter 4310: loss 0.2232, time 234.14ms, mfu 2.09%\n",
            "iter 4320: loss 0.2366, time 233.37ms, mfu 2.10%\n",
            "iter 4330: loss 0.2278, time 239.27ms, mfu 2.11%\n",
            "iter 4340: loss 0.2471, time 241.33ms, mfu 2.11%\n",
            "iter 4350: loss 0.2340, time 240.18ms, mfu 2.11%\n",
            "iter 4360: loss 0.2426, time 242.08ms, mfu 2.12%\n",
            "iter 4370: loss 0.2192, time 238.98ms, mfu 2.12%\n",
            "iter 4380: loss 0.1973, time 239.16ms, mfu 2.12%\n",
            "iter 4390: loss 0.2238, time 237.58ms, mfu 2.13%\n",
            "iter 4400: loss 0.2008, time 234.24ms, mfu 2.14%\n",
            "iter 4410: loss 0.2011, time 233.29ms, mfu 2.14%\n",
            "iter 4420: loss 0.2202, time 235.85ms, mfu 2.15%\n",
            "iter 4430: loss 0.2317, time 233.84ms, mfu 2.15%\n",
            "iter 4440: loss 0.2119, time 228.09ms, mfu 2.16%\n",
            "iter 4450: loss 0.2212, time 227.69ms, mfu 2.17%\n",
            "iter 4460: loss 0.2089, time 226.25ms, mfu 2.18%\n",
            "iter 4470: loss 0.2208, time 227.89ms, mfu 2.19%\n",
            "iter 4480: loss 0.1968, time 234.48ms, mfu 2.19%\n",
            "iter 4490: loss 0.2121, time 230.06ms, mfu 2.20%\n",
            "step 4500: train loss 0.0820, val loss 9.0985\n",
            "saving checkpoint to out\n",
            "iter 4500: loss 0.2289, time 29039.86ms, mfu 1.98%\n",
            "iter 4510: loss 0.2279, time 226.69ms, mfu 2.01%\n",
            "iter 4520: loss 0.2252, time 227.54ms, mfu 2.03%\n",
            "iter 4530: loss 0.2131, time 230.14ms, mfu 2.05%\n",
            "iter 4540: loss 0.2009, time 230.44ms, mfu 2.07%\n",
            "iter 4550: loss 0.1995, time 234.49ms, mfu 2.09%\n",
            "iter 4560: loss 0.2198, time 232.04ms, mfu 2.10%\n",
            "iter 4570: loss 0.2171, time 236.04ms, mfu 2.11%\n",
            "iter 4580: loss 0.2203, time 241.50ms, mfu 2.11%\n",
            "iter 4590: loss 0.2276, time 240.77ms, mfu 2.11%\n",
            "iter 4600: loss 0.2250, time 240.23ms, mfu 2.12%\n",
            "iter 4610: loss 0.2129, time 239.48ms, mfu 2.12%\n",
            "iter 4620: loss 0.2116, time 235.92ms, mfu 2.13%\n",
            "iter 4630: loss 0.2119, time 238.45ms, mfu 2.13%\n",
            "iter 4640: loss 0.1847, time 239.49ms, mfu 2.13%\n",
            "iter 4650: loss 0.2010, time 238.97ms, mfu 2.13%\n",
            "iter 4660: loss 0.2045, time 233.46ms, mfu 2.14%\n",
            "iter 4670: loss 0.2259, time 232.26ms, mfu 2.15%\n",
            "iter 4680: loss 0.2339, time 233.79ms, mfu 2.16%\n",
            "iter 4690: loss 0.2130, time 230.21ms, mfu 2.16%\n",
            "iter 4700: loss 0.2021, time 231.86ms, mfu 2.17%\n",
            "iter 4710: loss 0.1964, time 228.10ms, mfu 2.18%\n",
            "iter 4720: loss 0.1934, time 231.29ms, mfu 2.18%\n",
            "iter 4730: loss 0.2207, time 227.66ms, mfu 2.19%\n",
            "iter 4740: loss 0.2173, time 229.27ms, mfu 2.20%\n",
            "step 4750: train loss 0.0788, val loss 9.1534\n",
            "saving checkpoint to out\n",
            "iter 4750: loss 0.2067, time 28974.68ms, mfu 1.98%\n",
            "iter 4760: loss 0.2273, time 228.79ms, mfu 2.01%\n",
            "iter 4770: loss 0.2169, time 226.91ms, mfu 2.03%\n",
            "iter 4780: loss 0.1920, time 232.17ms, mfu 2.05%\n",
            "iter 4790: loss 0.1931, time 232.14ms, mfu 2.07%\n",
            "iter 4800: loss 0.1932, time 231.65ms, mfu 2.08%\n",
            "iter 4810: loss 0.1910, time 231.88ms, mfu 2.10%\n",
            "iter 4820: loss 0.2040, time 234.85ms, mfu 2.11%\n",
            "iter 4830: loss 0.1932, time 235.44ms, mfu 2.12%\n",
            "iter 4840: loss 0.1919, time 239.79ms, mfu 2.12%\n",
            "iter 4850: loss 0.1798, time 239.03ms, mfu 2.12%\n",
            "iter 4860: loss 0.2035, time 238.42ms, mfu 2.13%\n",
            "iter 4870: loss 0.1858, time 237.94ms, mfu 2.13%\n",
            "iter 4880: loss 0.1710, time 239.12ms, mfu 2.13%\n",
            "iter 4890: loss 0.2073, time 235.81ms, mfu 2.14%\n",
            "iter 4900: loss 0.1883, time 233.13ms, mfu 2.15%\n",
            "iter 4910: loss 0.1983, time 233.16ms, mfu 2.15%\n",
            "iter 4920: loss 0.2009, time 231.40ms, mfu 2.16%\n",
            "iter 4930: loss 0.1769, time 230.28ms, mfu 2.17%\n",
            "iter 4940: loss 0.1928, time 230.37ms, mfu 2.17%\n",
            "iter 4950: loss 0.1937, time 230.74ms, mfu 2.18%\n",
            "iter 4960: loss 0.1884, time 232.10ms, mfu 2.18%\n",
            "iter 4970: loss 0.1870, time 231.97ms, mfu 2.19%\n",
            "iter 4980: loss 0.2113, time 234.01ms, mfu 2.19%\n",
            "iter 4990: loss 0.1879, time 228.47ms, mfu 2.20%\n",
            "step 5000: train loss 0.0753, val loss 9.2211\n",
            "saving checkpoint to out\n",
            "iter 5000: loss 0.1878, time 28960.22ms, mfu 1.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Outputs with our New Model"
      ],
      "metadata": {
        "id": "L2J5JlRxFJOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation Set Up and Model Loading"
      ],
      "metadata": {
        "id": "eo_QP1ITFfX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "-vftqU9LheEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRB3j7iiNkl",
        "outputId": "9d408f0b-371f-477b-ba7f-e321faf6c557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ],
      "metadata": {
        "id": "N1YAy8DriVZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tokenizer\n",
        "encode = lambda s: enc.encode(s)\n",
        "decode = lambda l: enc.decode(l)"
      ],
      "metadata": {
        "id": "KoB-5ZuLicAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation!"
      ],
      "metadata": {
        "id": "mkTQ9wo7FjYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmTcaHCjii5l",
        "outputId": "0805f2cc-8b77-40e1-8b8f-d63835a1f4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To the swift ambassador,\n",
            "Where the swift ambassador,\n",
            "To a clear his bosom find\n",
            "Where you shall cross this tied in the rigour of severest law.\n",
            "\n",
            "PRINCE:\n",
            "We wakes; there's man? what can he say in this?\n",
            "Where is Romeo's Romeo's Romeo's man?? that Romeo? that kill'd Mercutio?\n",
            "\n",
            "BALTHASAR:\n",
            "I brought my master news of Juliet's death;\n",
            "And then in post he came from Mantua\n",
            "To this same place, to this same monument.\n",
            "This letter he early bid me give his father,\n",
            "And threatened me with death, going in the vault,\n",
            "I departed not and left him there.\n",
            "\n",
            "PRINCE:\n",
            "Give me the letter; I will look on it.\n",
            "Where is the county's page, that raised the watch?\n",
            "Sirrah, what made your master in this place?\n",
            "\n",
            "PAGE:\n",
            "He came with flowers to strew his lady's grave;\n",
            "And bid me stand aloof, and so I did:\n",
            "Anon comes one with light to ope the tomb;\n",
            "And by and by and by my master drew on him;\n",
            "And then I ran away to call the watch.\n",
            "\n",
            "PRINCE:\n",
            "This letter doth make good the friar's words,\n",
            "Their course of love, the tidings of her death:\n",
            "And here he writes that he did buy a poison\n",
            "Of a poor 'pothecary, and therewithal\n",
            "Came to this vault to die, and lie with Juliet.\n",
            "Where be these enemies? Capulet! Montague!\n",
            "See, what a scourge is laid upon your hate,\n",
            "That heaven finds means to kill your joys with love.\n",
            "And I for winking at your discords too\n",
            "Have lost a brace of kinsmen: all are punish'd.\n",
            "\n",
            "CAPULET:\n",
            "O brother Montague, give me thy hand:\n",
            "This is my daughter's jointure, for no more\n",
            "Can I demand.\n",
            "\n",
            "MONTAGUE:\n",
            "But I can give thee more:\n",
            "For I will raise her statue in pure gold;\n",
            "That while Verona by that name is known,\n",
            "There shall no figure at such rate be set\n",
            "As that of true and faithful Juliet.\n",
            "\n",
            "CAPULET:\n",
            "As rich shall Romeo's by his lady's lie;\n",
            "Poor sacrifices of our enmity!\n",
            "\n",
            "P\n",
            "---------------\n",
            "\n",
            "JULIET:\n",
            "Do not speak ill, how I am gone;\n",
            "And yet no pity sitting in the bottom of my face\n",
            "Or, sweet my mother, cast me not away!\n",
            "Delay this marriage for a month, a week;\n",
            "Or, if you do not, make the bridal bed\n",
            "In that dim monument where Tybalt lies.\n",
            "\n",
            "LADY CAPULET:\n",
            "Talk not to me, for I'll not speak a word:\n",
            "Do as thou wilt, for I have done with thee.\n",
            "\n",
            "JULIET:\n",
            "O God!--O nurse, how shall this be prevented?\n",
            "My husband is on earth, my faith in heaven;\n",
            "How shall that faith return again to earth,\n",
            "Unless that husband send it me from heaven\n",
            "By leaving earth? comfort me.\n",
            "Alack, alack, that heaven should practise stratagems\n",
            "Upon so soft a subject as myself!\n",
            "What say'st thou? hast thou not a word of joy?\n",
            "Some comfort, nurse.\n",
            "\n",
            "Nurse:\n",
            "Faith, here it is.\n",
            "Romeo is banish'd; and all the world to nothing,\n",
            "That he dares ne'er come back to challenge you;\n",
            "Or, if he do, it needs must be by stealth.\n",
            "Then, since the case so stands as now it doth,\n",
            "I think it best you married with the county.\n",
            "O, he's a lovely gentleman!\n",
            "Romeo's a dishclout to him: an eagle, madam,\n",
            "Hath not so green, so quick, so fair an eye\n",
            "As Paris hath. Beshrew my very heart,\n",
            "I think you are happy in this second match,\n",
            "For it excels your first: or if it did not,\n",
            "Your first is dead; or 'twere as good he were,\n",
            "As living here and you no use of him.\n",
            "\n",
            "JULIET:\n",
            "Speakest thou from thy heart?\n",
            "\n",
            "Nurse:\n",
            "And from my soul too;\n",
            "Or else beshrew them both.\n",
            "JULIET:\n",
            "Amen!\n",
            "\n",
            "Nurse:\n",
            "What?\n",
            "\n",
            "JULIET:\n",
            "Well, thou hast comforted me marvellous much.\n",
            "Go in: and tell my lady I am gone,\n",
            "Having displeased my father, to Laurence' cell,\n",
            "To make confession and to be absolved.\n",
            "\n",
            "Nurse:\n",
            "Marry, I will\n",
            "---------------\n",
            "\n",
            "ISABELLA:\n",
            "So you so your honour from the world no better.\n",
            "\n",
            "CLAUDIO:\n",
            "But is the judge, youf the judge, and free your brother's life,\n",
            "So every syllable a faithful verity:\n",
            "The duke comes home to-morrow; nay, dry your eyes;\n",
            "One of our convent, and his confessor,\n",
            "Gives me this instance: already he hath carried\n",
            "Notice to Escalus and Angelo,\n",
            "Who do prepare to meet him at the gates,\n",
            "There to give up their power. If you can, pace your wisdom\n",
            "In that good path that I would wish it go,\n",
            "And you shall have your bosom on this wretch,\n",
            "Grace of the duke, revenges to your heart,\n",
            "And general honour.\n",
            "\n",
            "ISABELLA:\n",
            "I am directed by you.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "This letter, then, to Friar Peter give;\n",
            "'Tis that he sent me of the duke's return:\n",
            "Say, by this token, I desire his company\n",
            "At Mariana's house to-night. Her cause and yours\n",
            "I'll perfect him withal, and he shall bring you\n",
            "Before the duke, and to the duke, and to the head of Angelo\n",
            "Accuse him home and home. For my poor self,\n",
            "I am combined by a sacred vow\n",
            "And shall be absent. Wend you with this letter:\n",
            "Command these fretting waters from your eyes\n",
            "With a light heart; trust not my holy order,\n",
            "If I pervert your course. Who's here?\n",
            "\n",
            "LUCIO:\n",
            "Good even. Friar, where's the provost?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Not within, sir.\n",
            "\n",
            "LUCIO:\n",
            "O pretty Isabella, I am pale at mine heart to see\n",
            "thine eyes so red: thou must be patient. I am fain\n",
            "to dine and sup with water and bran; I dare not for\n",
            "my head fill my belly; one fruitful meal would set\n",
            "me to 't. But they say the duke will be here\n",
            "to-morrow. By my troth, Isabel, I loved thy brother:\n",
            "if the old fantastical duke of dark corners had lived\n",
            "at home, he had lived.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Sir, the duke is marvellous little beholding to your\n",
            "reports; but the best is, he lives not in\n",
            "---------------\n",
            "\n",
            "\n",
            "KING RICHARD III:\n",
            "I am in haste. Hark, come hither, by the Earl of Richmond!\n",
            "What, will our helms?\n",
            "\n",
            "STANLEY:\n",
            "My lord, the western coast\n",
            "KING RICHARD III:\n",
            "Bishop, farewell:\n",
            "Post mighty sovereign, in arms.\n",
            "\n",
            "KING RICHARD III:\n",
            "O Buckingham, now! what do you?\n",
            "\n",
            "STANLEY:\n",
            "My lord, now on the chair empty?\n",
            "\n",
            "KING RICHARD III:\n",
            "And who is the sword unsway'd?\n",
            "Is the king dead? the empire unpossess'd?\n",
            "What heir of York is there alive but we?\n",
            "And who is England's king but great York's heir?\n",
            "Then, tell me, what doth he upon the sea?\n",
            "STANLEY:\n",
            "Unless for that, my liege, I cannot guess.\n",
            "\n",
            "KING RICHARD III:\n",
            "Unless for that he comes to be your liege,\n",
            "You cannot guess wherefore the Welshman comes.\n",
            "Thou wilt revolt, and fly to him, I fear.\n",
            "\n",
            "STANLEY:\n",
            "No, mighty liege; therefore mistrust me not.\n",
            "\n",
            "KING RICHARD III:\n",
            "Where is thy power, then, to beat him back?\n",
            "Where are thy tenants and thy followers?\n",
            "Are they not now upon the western shore.\n",
            "Safe-conducting the rebels from their ships!\n",
            "\n",
            "STANLEY:\n",
            "No, my good lord, my friends are in the north.\n",
            "\n",
            "KING RICHARD III:\n",
            "Cold friends to Richard: what do they in the north,\n",
            "When they should serve their sovereign in the west?\n",
            "\n",
            "STANLEY:\n",
            "They have not been commanded, mighty sovereign:\n",
            "Please it your majesty to give me leave,\n",
            "I'll muster up my friends, and meet your grace\n",
            "Where and what time your majesty shall please.\n",
            "\n",
            "KING RICHARD III:\n",
            "Ay, ay. thou wouldst be gone to join with Richmond:\n",
            "I will not trust you, sir.\n",
            "\n",
            "STANLEY:\n",
            "Most mighty sovereign,\n",
            "You have no cause to hold my friendship doubtful:\n",
            "I never was nor never will be false.\n",
            "\n",
            "KING RICHARD III:\n",
            "Well,\n",
            "Go muster men; but, hear you, leave behind\n",
            "Your son, George Stanley: look your faith be firm.\n",
            "Or else his head's assurance is but frail.\n",
            "\n",
            "STANLEY:\n",
            "So deal with him as\n",
            "---------------\n",
            "\n",
            "\n",
            "JULIET:\n",
            "I do; and I joy, for the friend.\n",
            "\n",
            "ROMEO:\n",
            "Let me stand here till thou remember it.\n",
            "\n",
            "JULIET:\n",
            "I shall forget, to have thee still forget,\n",
            "Forgetting any other home but this.\n",
            "\n",
            "ROMEO:\n",
            "'Tis almost morning; I would have thee gone:\n",
            "And yet no further than a wanton's bird;\n",
            "Who lets it hop a little from her hand,\n",
            "Like a poor prisoner in his twisted gyves,\n",
            "And with a silk thread plucks it back again,\n",
            "So loving-jealous of his liberty.\n",
            "\n",
            "JULIET:\n",
            "I would I were thy bird.\n",
            "\n",
            "ROMEO:\n",
            "Sweet, so would I:\n",
            "Yet I:\n",
            "Good night, so would I:\n",
            "For in his morrow,\n",
            " parting is such\n",
            "As is such merchandise.\n",
            "\n",
            "JULIET:\n",
            "O, good night is too late!\n",
            "\n",
            "ROMEO:\n",
            "That our time to-morrow\n",
            "That we on my greetings to-night\n",
            "\n",
            "JULIET:\n",
            "At thelse would I tear the mask of night is out the world:\n",
            "My name that we have an hundred thousand times of strength,\n",
            "And I have not believe to thee.\n",
            "\n",
            "ROMEO:\n",
            "I doubt it were to-night: 'tis twenty years to-night\n",
            "Thou art thyself, for the mask of night'st my idolatry,\n",
            "And I'll believe thee.\n",
            "\n",
            "JULIET:\n",
            "I would it were night's dear love's dear love's dear love's dear love:\n",
            "It is too unadvised, too sudden;\n",
            "Too like the lightning, which doth cease to be\n",
            "Ere one can say 'It lightens.' Sweet, good night!\n",
            "This bud of love, by summer's ripening breath,\n",
            "May prove a beauteous flower when next we meet.\n",
            "Good night, good night! as sweet repose and rest\n",
            "Come to thy heart as that within my breast!\n",
            "\n",
            "ROMEO:\n",
            "O, wilt thou leave me so unsatisfied?\n",
            "\n",
            "JULIET:\n",
            "What satisfaction canst thou have to-night?\n",
            "\n",
            "ROMEO:\n",
            "The exchange of thy love's faithful vow for mine.\n",
            "\n",
            "JULIET:\n",
            "I gave thee mine before thou didst request it:\n",
            "And\n",
            "---------------\n",
            "\n",
            "If I should have been so long.\n",
            "As merry as sweet a mockery king of France,\n",
            "To revel it with the English crown, the English crown,\n",
            "And says that Clarence' was the Lady Bona.\n",
            "\n",
            "RIVERS:\n",
            "Why, madam?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "What were you were you and I:\n",
            "She may do the king?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "I blame, I pity not: I pity the Earl of Warwick's corse.\n",
            "\n",
            "GLOUCESTER:\n",
            "Nor none that I cursed myself.\n",
            "\n",
            "RIVERS:\n",
            "I told you have all harm.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "The king that I hear of us confer with a wonder:\n",
            "Betweet prince will not on the Duke of York,\n",
            "They'll corrupt her husband's govern'd by women proud adversaries,\n",
            "That I, to Lady Grey:\n",
            "Your mother, and your minds,\n",
            "And the Lady Grey, they fall'n pure heart's queen;\n",
            "With some reason may win her, and your forgery and his,\n",
            "Sends me a paper to persuade me patience?\n",
            "Dare not become my vow not become her father.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "King Lewis, what are thou,\n",
            "Thy sly conveyance and thy fair sister were I'll aid.\n",
            "\n",
            "GLOUCESTER:\n",
            "If she were no more, but what you are not on your off.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "But love of my lord, and you are not worse,\n",
            "Thy other shape is not worse,\n",
            "And little joy of a direful pageant;\n",
            "One heaved a-high, to be hurl'd down below;\n",
            "A mother only mock'd with two sweet babes;\n",
            "A mother only mock'd with two sweet babes;\n",
            "A mother only mock'd with two sweet babes;\n",
            "A mother only mock'd with two sweet babes;\n",
            "Give mine.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "How will were destined to a lord?\n",
            "\n",
            "DORSET:\n",
            "If grace my lord,\n",
            "If grace my mother, not you call'd withal;\n",
            "You have no thankings, nor, nor no prouds,\n",
            "But fettle your fine joints 'gainst Thursday next,\n",
            "To go with Paris to Saint Peter's Church,\n",
            "Or I will drag thee on a hurdle thither.\n",
            "Out, you green-\n",
            "---------------\n",
            "\n",
            "The slave, and am made me.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Boldly done so dishonour'd, and greatness\n",
            "I have got them in the wards of covert bosom,\n",
            "When it deserves, with characters of brass,\n",
            "A forted residence 'gainst the tooth of time\n",
            "And razure of oblivion. Give me your hand,\n",
            "And let the subject see, to make them know\n",
            "That outward courtesies would fain proclaim\n",
            "Favours that keep within. Come, Escalus,\n",
            "You must walk by us on our other hand;\n",
            "And good supporters are you.\n",
            "\n",
            "FRIAR PETER:\n",
            "Now is your time: speak loud and kneel before him.\n",
            "\n",
            "ISABELLA:\n",
            "Justice, O royal duke! Vail your regard\n",
            "Upon a wrong'd, I would fain have said, a maid!\n",
            "O worthy prince, dishonour not your eye\n",
            "By throwing it on any other object\n",
            "Till you have heard me in my true complaint\n",
            "And given me justice, justice, justice!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Relate your wrongs; in what? by whom? be brief.\n",
            "Here is Lord Angelo shall give you justice:\n",
            "Reveal yourself to him.\n",
            "\n",
            "ISABELLA:\n",
            "O worthy duke,\n",
            "You bid me seek redemption of the devil:\n",
            "Hear me yourself; for that which I must speak\n",
            "Must either punish me, not being believed,\n",
            "Or wring redress from you. Hear me, O hear me, here!\n",
            "\n",
            "ANGELO:\n",
            "My lord, her wits, her wits, I fear me, are not firm:\n",
            "She hath been a suitor to me for her brother\n",
            "Cut off by course of justice,--\n",
            "\n",
            "ISABELLA:\n",
            "By course of justice!\n",
            "\n",
            "ANGELO:\n",
            "And she will speak most bitterly and strange.\n",
            "\n",
            "ISABELLA:\n",
            "Most strange, but yet most truly, will I speak:\n",
            "That Angelo's forsworn; is it not strange?\n",
            "That Angelo is 't not strange?\n",
            "That Angelo is an adulterous thief,\n",
            "An hypocrite, a virgin-violator;\n",
            "Is it not strange and strange?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Nay, it is ten times strange.\n",
            "\n",
            "ISABELLA:\n",
            "It is not truer he is Angelo\n",
            "Than this is all as it is strange:\n",
            "Nay, it is ten\n",
            "---------------\n",
            "\n",
            "I could make thy beauty's wreck;\n",
            "For beauty that still live in all mischance.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Be plain, but not on thy breast shall rue the heart.\n",
            "\n",
            "KING LEWIS XI:\n",
            "Now, sister, let us hear your king's king's;\n",
            "And sit aside, and Margaret:\n",
            "Yield not on Edward's fruit,\n",
            "And says that by Edward's good to the way;\n",
            "What answer shall you?\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Our Earl of Warwick! What brings thee to rise;\n",
            "For this is he that moves both wind and tide?\n",
            "\n",
            "WARWICK:\n",
            "From worthy Edward, King of Albion,\n",
            "My lord and thy vowed friend,\n",
            "I come, in kindness and unfeigned love,\n",
            "First, to do greetings to thy royal person;\n",
            "And then to crave a league of amity;\n",
            "And lastly, to confirm that amity\n",
            "With a nuptial knot, if thou vouchsafe to grant\n",
            "That virtuous Lady Bona, thy fair sister,\n",
            "To England's king in lawful marriage.\n",
            "QUEEN MARGARET:\n",
            "\n",
            "WARWICK:\n",
            "\n",
            "QUEEN MARGARET:\n",
            "King Lewis and Lady Bona,\n",
            "Before you answer Warwick. His demand\n",
            "Springs not from Edward's well-meant honest love,\n",
            "But from deceit bred by necessity;\n",
            "For how can tyrants safely govern home,\n",
            "Unless abroad they purchase great alliance?\n",
            "To prove him tyrant this reason may suffice,\n",
            "That Henry liveth still: but were he dead,\n",
            "Yet here Prince Edward stands, King Henry's son.\n",
            "Look, therefore, Lewis, Lewis, that by this league and marriage\n",
            "Thou draw not on thy danger and dishonour;\n",
            "For though usurpers sway the rule awhile,\n",
            "Yet heavens are just, and time suppresseth wrongs.\n",
            "\n",
            "WARWICK:\n",
            "Injurious Margaret!\n",
            "\n",
            "PRINCE EDWARD:\n",
            "And why not queen?\n",
            "\n",
            "Because thy father Henry did usurp;\n",
            "And thou no more are prince than she is queen.\n",
            "\n",
            "WARWICK:\n",
            "Because thy father Henry did usurp;\n",
            "Then Warwick disannuls great John of Gaunt,\n",
            "Which did subdue the greatest part of Spain;\n",
            "And, after John of Gaunt, Henry the Fourth,\n",
            "Whose wisdom was a mirror to the wisest;\n",
            "And, after that wise prince, Henry the Fifth\n",
            "---------------\n",
            "\n",
            "LEONTES:\n",
            "O Paulina,\n",
            "I know not how I am sorry\n",
            "Hermione was about her good Paulina, nothing\n",
            "As was about her good Paulina,\n",
            "So aged as this seems.\n",
            "\n",
            "PAULINA:\n",
            "O,\n",
            "So much wrinkled, not by much.\n",
            "\n",
            "LEONTES:\n",
            "As now much the more our carver's excellence;\n",
            "Which lets go by some sixteen years and makes her\n",
            "As she lived now.\n",
            "\n",
            "PAULINA:\n",
            "So much to my good comfort,\n",
            "So much to my good comfort, as it is\n",
            "So much to my good comfort, as it is\n",
            "Now piercing to my soul. O, thus she stood,\n",
            "Even with such life of majesty, warm life,\n",
            "As now it coldly stands, when first I woo'd her!\n",
            "I am ashamed: does not the stone rebuke me\n",
            "For being more stone than it? O royal piece,\n",
            "There's magic in thy majesty, which has\n",
            "My evils conjured to remembrance and\n",
            "From thy admiring daughter took the spirits,\n",
            "Standing like stone with thee.\n",
            "\n",
            "PERDITA:\n",
            "And give them.\n",
            "\n",
            "PAULINA:\n",
            "Either forbear,\n",
            "And yet\n",
            "Quit presently the chapel, or resolve you\n",
            "For more amazement. If you can behold it,\n",
            "I'll make the statue move indeed, descend\n",
            "And take you by the hand; but then you'll think--I am assisted\n",
            "By wicked powers.\n",
            "\n",
            "LEONTES:\n",
            "What you can make her do,\n",
            "I am content to look on: what to speak,\n",
            "I am content to hear; for 'tis as easy\n",
            "To make her speak as move.\n",
            "\n",
            "PAULINA:\n",
            "It is required\n",
            "You do awake your faith. Then all stand still;\n",
            "On: those that think it is unlawful business\n",
            "I am about, let them depart.\n",
            "\n",
            "LEONTES:\n",
            "Proceed:\n",
            "No foot shall stir.\n",
            "\n",
            "PAULINA:\n",
            "Music, awake her; strike!\n",
            "'Tis time; descend; be stone no more; approach;\n",
            "Strike all that look upon with marvel. Come,\n",
            "I'll fill your grave up: stir, nay, nay,\n",
            "Bequeath to death your numbness, for from him\n",
            "Dear life redeems you. You perceive she stirs:\n",
            "Start\n",
            "---------------\n",
            "\n",
            "Not to be so much.\n",
            "\n",
            "LEONTES:\n",
            "There is no more: when I\n",
            "I' fecks!\n",
            "Why, that's my bawcock. What, hast\n",
            "smutch'd thy nose?\n",
            "They say it is a copy out of mine. Come, captain,\n",
            "We must be neat; not neat, but cleanly, captain:\n",
            "And yet the steer, the heifer and the calf\n",
            "Are all call'd neat.--Still virginalling\n",
            "Upon his palm!--How now, you wanton calf!\n",
            "Art thou my calf?\n",
            "\n",
            "MAMILLIUS:\n",
            "Yes, if you will, my lord.\n",
            "\n",
            "LEONTES:\n",
            "Thou want'st a rough pash and the shoots that I have,\n",
            "To be full like me: yet they say we are\n",
            "Almost as eggs; women say so,\n",
            "That will say so, as eggs; women say so,\n",
            "As o'er-dyed blacks, as wind, as wind, false\n",
            "As dice are to be wish'd by one that fixes\n",
            "No bourn 'twixt his and mine, yet were it true\n",
            "To say this boy were like me. Come, sir page,\n",
            "Look on me with your welkin eye: sweet villain!\n",
            "Most dear'st! my collop! Can thy dam?--may't be?--may't be?--\n",
            "Affection! thy intention stabs the centre:\n",
            "Thou dost make possible things not so held,\n",
            "Communicatest with dreams;--how can this be?--\n",
            "With what's unreal thou coactive art,\n",
            "And fellow'st nothing: then 'tis very credent\n",
            "Thou mayst co-join with something; and thou dost,\n",
            "And that beyond commission, and I find it,\n",
            "And that to the infection of my brains\n",
            "And hardening of my brows.\n",
            "\n",
            "POLIXENES:\n",
            "What means Sicilia?\n",
            "\n",
            "HERMIONE:\n",
            "He something seems unsettled.\n",
            "\n",
            "POLIXENES:\n",
            "How, my lord!\n",
            "What cheer? how is't with you, best brother?\n",
            "\n",
            "HERMIONE:\n",
            "You look as if you held a brow of much distraction\n",
            "Are you moved, my lord?\n",
            "\n",
            "LEONTES:\n",
            "No, in good earnest.\n",
            "How sometimes nature will betray its folly,\n",
            "Its tenderness, and make itself a pastime\n",
            "To harder bosoms! Looking on the lines\n",
            "Of\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}